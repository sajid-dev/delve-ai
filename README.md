# delve-ai

A productionâ€‘grade backend for a chat application powered by large language models.  It exposes a REST API using **FastAPI** and uses **LangChain** to manage communication with your chosen model via a persistent **ChromaDB** vector store.  Structured logging is handled via **Loguru**.

## Features

The **delveâ€‘ai** backend is designed as a productionâ€‘ready foundation for enterprise chat applications.  In addition to the basic chat and health endpoints, the architecture provides a rich feature set across multiple domains.  Some capabilities are fully implemented today, while others are designed into the architecture and can be enabled with additional configuration or plugins.

### Core API endpoints

- **Chat endpoint** â€“ `POST /chat` accepts a prompt and returns an AI response.
- **Admin dashboard** â€“ `GET /admin/dashboard` reports per-user conversations with aggregated token usage.
- **Health check** â€“ `GET /health` responds with the current health of the service.
- **LangChain integration** â€“ uses `langchain` and `langchainâ€‘openai` for conversation chains backed by a `ChromaDB` vector store.
- **Structured logging** â€“ uses Loguru with log rotation and retention to persist logs in `logs/app.log`.
- **Configuration via environment variables** â€“ secrets and tunables are loaded from a `.env` file via Pydantic settings.

### Extended capabilities

Below is an overview of additional features supported by the architecture.  Items marked *ready* are planned and the design accommodates them, but they may require further implementation or configuration to activate.

#### ğŸ’¾ Memory & Storage

- **ChromaDB integration** â€“ persistent vector database for conversation storage (implemented).
- **Userâ€‘based isolation** â€“ complete separation of user conversations (*ready*).
- **Conversation persistence** â€“ automatic saving of chat history (implemented).
- **Vector similarity search** â€“ semantic search through past conversations (implemented via `Chroma` retriever).
- **Multiple storage backâ€‘ends** â€“ architecture supports alternative stores such as Redis or PostgreSQL (*ready*).
- **Automatic data persistence** â€“ no manual saving required (implemented).

#### ğŸ”„ Conversation management

- **Multiple conversations** â€“ ability for users to have multiple simultaneous chats (*ready*; requires a user/memory manager).
- **Conversation history** â€“ full message history retrieval (implemented).
- **Contextâ€‘aware responses** â€“ uses previous conversation context (implemented).
- **Conversation metadata** â€“ titles, timestamps, message counts (*ready*).
- **Conversation deletion** â€“ individual conversation removal (*ready*).
- **Bulk operations** â€“ clear all user history (*ready*).

#### ğŸ‘¥ User management

- **Automatic user ID generation** â€“ unique user identification (*ready*).
- **Session persistence** â€“ maintains state across sessions (implemented via persistent storage).
- **User data isolation** â€“ complete privacy between users (*ready*; requires user memory manager).
- **Provider-aware attribution** â€“ forwards the supplied `user_id` to the LLM for auditing and usage tracking (implemented).
- **User preference storage** â€“ architecture for user settings (*ready*).
- **Authentication ready** â€“ prepared for user login systems (*ready*).

#### âš™ï¸ Configuration & settings

- **Environmentâ€‘based configuration** â€“ `.env` file support (implemented).
- **Runtime configuration** â€“ hotâ€‘reload capable settings (*ready*).
- **Multiple environment support** â€“ dev, staging, production (implemented).
- **Centralised configuration** â€“ single source for all settings (implemented via `src/config`).
- **Validation** â€“ typeâ€‘safe configuration management using Pydantic (implemented).

#### ğŸ›¡ï¸ Error handling & reliability

- **Comprehensive error handling** â€“ graceful degradation on failures (implemented via custom exceptions).
- **Automatic retry logic** â€“ retry failed API calls (*ready*; would be implemented at the API client layer).
- **Circuit breaker pattern** â€“ prevents cascading failures (*ready*).
- **Fallback mechanisms** â€“ continue working during partial outages (*ready*).
- **Detailed error logging** â€“ complete error context for debugging (implemented via Loguru).
- **Userâ€‘friendly error messages** â€“ appropriate feedback for users (implemented).

#### ğŸ“Š Monitoring & analytics

- **Service health checks** â€“ comprehensive system monitoring (implemented).
- **Conversation analytics dashboard** â€“ built-in admin endpoint summarising usage across users (implemented).
- **Performance metrics** â€“ response times, success rates (*ready*).
- **Usage statistics** â€“ conversation counts, message volumes (*ready*).
- **Logging infrastructure** â€“ structured logging with multiple levels (implemented).
- **Audit trail** â€“ complete operation logging (*ready*).

#### ğŸ”§ API & integration

- **REST API ready** â€“ full API endpoint architecture (implemented).
- **Webhook support ready** â€“ prepared for external integrations (*ready*).
- **Batch processing support** â€“ bulk operations capability (*ready*).
- **Custom prompt engineering** â€“ support for specialised prompts (implemented via LangChain chains).
- **Multiâ€‘modal ready** â€“ architecture prepared for images/audio (*ready*).

#### ğŸš€ Performance & scalability

- **Caching mechanisms** â€“ performance optimisation (*ready*).
- **Connection pooling** â€“ efficient resource usage (*ready*; depends on HTTP client).
- **Horizontal scaling ready** â€“ stateless service design (implemented).
- **Load balancer compatible** â€“ production deployment ready (implemented).
- **Resource optimisation** â€“ efficient memory and CPU usage (implemented via asynchronous FastAPI design).

#### ğŸ§ª Development & testing

- **Comprehensive test suite** â€“ unit, integration, endâ€‘toâ€‘end tests (*ready*; test scaffolding can be added).
- **Type safety** â€“ full type annotations throughout (implemented).
- **Debug mode** â€“ development debugging support (implemented via `app_debug`).
- **Mock services** â€“ testing without external dependencies (*ready*).
- **Documentation ready** â€“ comprehensive code documentation (implemented in `src/docs`).

#### ğŸ”’ Security & compliance

- **API key security** â€“ environment variable protection (implemented; secrets loaded via `.env`).
- **Input validation** â€“ SQL injection and XSS prevention (*ready*; depending on storage implementation).
- **Data encryption ready** â€“ prepared for encrypted storage (*ready*).
- **Privacy by design** â€“ user data isolation builtâ€‘in (*ready*).
- **Audit logging** â€“ compliance and security auditing (*ready*).

#### ğŸŒ Deployment & operations

- **Docker ready** â€“ containerisation support (*ready*; Dockerfile can be added for container builds).
- **Cloud platform compatible** â€“ AWS, GCP, Azure (implemented; FastAPI is platformâ€‘agnostic).
- **Streamlit sharing ready** â€“ oneâ€‘click deployment (*ready*).
- **Traditional server support** â€“ WSGI/ASGI compatible (implemented via Uvicorn/ASGI).
- **Zeroâ€‘downtime deployment ready** â€“ blueâ€‘green deployment capable (*ready*; requires deployment orchestration).

#### ğŸ”„ Workflow features

- **Endâ€‘toâ€‘end processing** â€“ complete message handling pipeline (implemented).
- **Memory integration** â€“ automatic context management (implemented).
- **Response generation** â€“ AI response creation and formatting (implemented via LangChain).
- **Continuous conversation** â€“ multiâ€‘turn dialogue support (implemented).
- **Context window management** â€“ smart context truncation (*ready*).

#### ğŸ“ˆ Advanced features

- **Conversation summarisation ready** â€“ architecture for summary generation (*ready*).
- **Knowledge base integration ready** â€“ prepared for external data sources (*ready*).
- **Custom embedding support** â€“ multiple embedding model compatibility (*ready*; via `OpenAIEmbeddings` parameters).
- **Semantic search** â€“ intelligent conversation retrieval (implemented via vector similarity search).
- **Personalisation ready** â€“ userâ€‘specific behaviour adaptation (*ready*).

## Getting started

1. **Clone the repository** and change into the backend directory:

   ```bash
   git clone <yourâ€‘repo>
   cd delve-ai
   ```

2. **Install uv and set up your project environment**.

   This project is designed to work seamlessly with the [uv](https://github.com/astral-sh/uv) Python project manager.  If you don't already have uv installed, you can install it with pip:

   ```bash
   pip install uv
   ```

   Once uv is installed, run the following command in the project root to create a `.venv` directory and install all dependencies defined in `pyproject.toml`:

   ```bash
   uv sync
   ```

   The first time you run `uv sync` it will create a virtual environment in `.venv/` and generate a `uv.lock` lockfile.  If you prefer to use a traditional virtual environment without uv, you can still create one manually:

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install .
   ```

3. **Configure environment variables**.  Copy `.env.example` to `.env` and fill in your language model credentials and settings.  At a minimum you must provide `LLM_API_KEY`.  You can optionally override `LLM_BASE_URL`, `LLM_MODEL` and other tuning parameters:

   ```bash
   cp .env.example .env
   # Then edit .env and set LLM_API_KEY, LLM_BASE_URL, LLM_MODEL, etc.
   ```

4. **Run the server**.  If you're using uv, you can run the FastAPI application directly in the uv-managed environment:

   ```bash
   uv run -- uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
   ```

   Alternatively, if you activated a virtual environment manually, run:

   ```bash
   uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
   ```

5. **Test the API**.  Use `curl` or a REST client to send a chat request:

   ```bash
   curl -X POST http://localhost:8000/chat \
     -H "Content-Type: application/json" \
     -d '{"message":"Hello"}'
   ```

   and check the health endpoint:

   ```bash
   curl http://localhost:8000/health
   ```

   Admin operators can inspect usage analytics:

   ```bash
   curl http://localhost:8000/admin/dashboard
   ```

## Project structure

```
delve-ai/
â”œâ”€â”€ chroma_db/          # Persistent vector store for ChromaDB
â”œâ”€â”€ .python-version     # Python version pin for uv-managed environments
â”œâ”€â”€ logs/               # Log files written by Loguru
â”œâ”€â”€ src/                # Source code organised by domain
â”‚   â”œâ”€â”€ main.py         # FastAPI application entry point
â”‚   â”œâ”€â”€ config/         # Configuration classes including settings.py, app_config.py, llm_config.py, logging_config.py
â”‚   â”œâ”€â”€ controllers/    # API controllers
â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â”œâ”€â”€ models/         # Pydantic models for requests and responses
â”‚   â”œâ”€â”€ memory/         # Conversation memory management
â”‚   â””â”€â”€ utils/          # Helper functions, error handling and logging
```

## Notes

* This backend uses `VectorStoreRetrieverMemory` from LangChain to store conversation history in a persistent `ChromaDB` database.  The database persists under the `chroma_db/` directory so that previous messages are used to provide context to subsequent prompts.
* Make sure the `LLM_API_KEY` environment variable is set; without it, the service will return an error.  If you are using a provider other than OpenAI, also set `LLM_BASE_URL` and the appropriate `LLM_MODEL`.

## API endpoints

The backend exposes a small number of HTTP endpoints via FastAPI.  All responses are JSON encoded.

### `GET /health`

Returns a simple health check indicating the service is running.

**Request:**

```
GET /health
```

**Response:**

```json
{
  "status": "ok"
}
```

### `POST /chat`

Starts or continues a conversation and returns an assistant response.  The request body must include a non-empty `message` field.  Optional `user_id` and `conversation_id` fields allow the client to specify an existing user and conversation.  If omitted, new identifiers are generated and returned in the response.  Input validation is handled by Pydantic; if the `message` field is missing, empty or exceeds 500 characters, FastAPI will return a 422 error.

When a `user_id` is present it is forwarded to the underlying language model, enabling per-user analytics, auditing or quota management at the model provider level.

**Request:**

```json
{
  "message": "Hello, who are you?",
  "user_id": "123e4567-e89b-12d3-a456-426614174000",        // optional
  "conversation_id": "9c5f869a-2b8a-47f6-9ffd-0cbbd9e02c66" // optional
}
```

**Response (200):**

```json
{
  "user_id": "123e4567-e89b-12d3-a456-426614174000",
  "conversation_id": "9c5f869a-2b8a-47f6-9ffd-0cbbd9e02c66",
  "answer": "Hello! I'm your AI assistant. How can I help you today?"
}
```

If the LLM or memory backend fails, the service returns a 500 Internal Server Error with a JSON body such as:

```json
{
  "detail": "LLM generation failed"
}
```

### `GET /conversations`

Lists all conversations for a given user.  The `user_id` query parameter is required.

**Request:**

```
GET /conversations?user_id=123e4567-e89b-12d3-a456-426614174000
```

**Response (200):**

```json
[
  {
    "conversation_id": "9c5f869a-2b8a-47f6-9ffd-0cbbd9e02c66",
    "user_id": "123e4567-e89b-12d3-a456-426614174000",
    "title": "Hello, who are you?",           // optional title derived from first message
    "created_at": "2025-09-17T10:00:00Z",
    "updated_at": "2025-09-17T10:01:00Z",
    "message_count": 2,
    "messages": [ /* omitted for brevity */ ]
  }
]
```

### `GET /conversations/{conversation_id}`

Retrieves a single conversation for a user.  Both `conversation_id` (path) and `user_id` (query) parameters are required.

**Request:**

```
GET /conversations/9c5f869a-2b8a-47f6-9ffd-0cbbd9e02c66?user_id=123e4567-e89b-12d3-a456-426614174000
```

**Response (200):**

```json
{
  "conversation_id": "9c5f869a-2b8a-47f6-9ffd-0cbbd9e02c66",
  "user_id": "123e4567-e89b-12d3-a456-426614174000",
  "title": "Hello, who are you?",
  "created_at": "2025-09-17T10:00:00Z",
  "updated_at": "2025-09-17T10:01:00Z",
  "message_count": 2,
  "messages": [
    { "role": "user", "content": "Hello, who are you?", "timestamp": "2025-09-17T10:00:00Z" },
    { "role": "assistant", "content": "Hello! I'm your AI assistant. How can I help you today?", "timestamp": "2025-09-17T10:01:00Z" }
  ]
}
```

If the conversation does not exist, the service returns a 404 Not Found.

### `DELETE /conversations/{conversation_id}`

Deletes a specific conversation for a user.  Requires both the `conversation_id` path parameter and the `user_id` query parameter.  Returns a 204 No Content response on success.

### `DELETE /conversations`

Deletes all conversations for a user.  Requires the `user_id` query parameter.  Returns a 204 No Content response on success.
