## Environment configuration for delve-ai
# Copy this file to `.env` and fill in the values for your environment.

# -----------------------------------------------------------------------------
# Language model configuration
#
# The backend uses a single set of environment variables to configure the
# language model (LLM) regardless of provider.  At a minimum you must
# specify the API key (``LLM_API_KEY``).  You can optionally override the
# base URL, model name and tuning parameters.  If ``LLM_BASE_URL`` is left
# blank the default OpenAI endpoints will be used【149861662473305†L170-L184】.

# API key used to authenticate with your LLM provider (e.g. OpenAI, LLaMA)
LLM_API_KEY=your-api-key

# Base URL for the API.  Leave empty to use the default OpenAI base URL.
LLM_BASE_URL=

# Name of the chat model to use (e.g. gpt-3.5-turbo, gpt-4-turbo, llama-2-7b-chat).
LLM_MODEL=gpt-3.5-turbo

# Temperature controls the creativity of responses; 0.0 is deterministic.
LLM_TEMPERATURE=0.7

# Maximum number of tokens to generate per response.
LLM_MAX_TOKENS=2048

# Timeout in seconds for API requests.
LLM_TIMEOUT=30

# Application Settings
APP_ENV=development
APP_DEBUG=true
APP_HOST=0.0.0.0
APP_PORT=8501

# Memory Configuration
MEMORY_TYPE=in_memory
# REDIS_URL=redis://localhost:6379/0  # Uncomment if using Redis

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/app.log