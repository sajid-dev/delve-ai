## Environment configuration for delve-ai
# Copy this file to `.env` and fill in the values for your environment.

# -----------------------------------------------------------------------------
# Language model configuration
#
# The backend uses a single set of environment variables to configure the
# language model (LLM) regardless of provider.  At a minimum you must
# specify the API key (``LLM_API_KEY``).  You can optionally override the
# base URL, model name and tuning parameters.  If ``LLM_BASE_URL`` is left
# blank the default OpenAI endpoints will be used【149861662473305†L170-L184】.

# API key used to authenticate with your LLM provider (e.g. OpenAI, LLaMA)
LLM_API_KEY=your-api-key

# Base URL for the API.  Leave empty to use the default OpenAI base URL.
LLM_BASE_URL=

# Name of the chat model to use (e.g. gpt-3.5-turbo, gpt-4-turbo, llama-2-7b-chat).
LLM_MODEL=gpt-3.5-turbo

# Temperature controls the creativity of responses; 0.0 is deterministic.
LLM_TEMPERATURE=0.7

# Maximum number of tokens to generate per response.
LLM_MAX_TOKENS=2048

# Timeout in seconds for API requests.
LLM_TIMEOUT=30

# Enable Model Context Protocol tooling support. When enabled, the backend
# will launch the specified MCP server and expose its tools to the LLM via
# LangChain's MCP client. Set to "true" to enable MCP.
LLM_MCP_ENABLED=false

# MCP transport to use. Only "stdio" is currently supported.
LLM_MCP_TRANSPORT=stdio

# Command used to start the MCP server. Required when MCP is enabled.
# Example: "uv" or "/usr/local/bin/python"
LLM_MCP_SERVER_COMMAND=

# Optional arguments passed to the MCP server command. Provide either a
# whitespace-separated string or JSON list, e.g. "run -- python -m my_server".
LLM_MCP_SERVER_ARGS=

# Optional JSON object of environment variables to inject when starting the server.
# Example: {"OPENAI_API_KEY":"your-key"}
LLM_MCP_SERVER_ENV=

# Optional working directory for the MCP server process.
LLM_MCP_SERVER_CWD=

# Optional keywords that decide when to call MCP tools. Space or comma separated.
# Example: "analytics finance report"
LLM_MCP_TRIGGER_KEYWORDS=

# Application Settings
APP_ENV=development
APP_DEBUG=true
APP_HOST=0.0.0.0
APP_PORT=8501

# Memory Configuration
MEMORY_TYPE=in_memory
# REDIS_URL=redis://localhost:6379/0  # Uncomment if using Redis

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/app.log
